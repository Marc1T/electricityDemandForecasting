{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6712b62f",
   "metadata": {},
   "source": [
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68ceb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e4a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_config(config_path: str = \"config.yaml\") -> dict:\n",
    "    \"\"\"\n",
    "    Lit le fichier YAML de configuration et renvoie un dict.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    # Remplace ${VARNAME} par la vraie variable d'env si nécessaire\n",
    "    for section in cfg.values():\n",
    "        if isinstance(section, dict):\n",
    "            for k, v in section.items():\n",
    "                if isinstance(v, str) and v.startswith(\"${\") and v.endswith(\"}\"):\n",
    "                    var = v[2:-1]\n",
    "                    section[k] = os.getenv(var)\n",
    "    return cfg\n",
    "\n",
    "# Charger config\n",
    "cfg = load_config(\"config.yaml\")\n",
    "raw_dir = Path(cfg[\"paths\"][\"data\"][\"raw\"])\n",
    "ext_dir = Path(cfg[\"paths\"][\"data\"][\"external\"])\n",
    "inter_dir = Path(cfg[\"paths\"][\"data\"][\"interim\"])\n",
    "inter_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Liste des zones\n",
    "zones = list(cfg[\"geo_zones\"].values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944670f",
   "metadata": {},
   "source": [
    "## 2. Loader Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141a96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_region = {\n",
    "    \"Gran_canaria\":             \"Canarias\",\n",
    "    \"Lanzarote_Fuerteventura\":  \"Canarias\",\n",
    "    \"Tenerife\":                 \"Canarias\",\n",
    "    \"La_Palma\":                 \"Canarias\",\n",
    "    \"La_Gomera\":                \"Canarias\",\n",
    "    \"El_Hierro\":                \"Canarias\",\n",
    "    \"Mallorca_Menorca\":         \"Baleares\",\n",
    "    \"Ibiza_Formentera\":         \"Baleares\",\n",
    "    # Les autres sont directes\n",
    "    \"Peninsule_Iberique\":       \"Peninsule_Iberique\",\n",
    "    \"Canarias\":                 \"Canarias\",\n",
    "    \"Baleares\":                 \"Baleares\",\n",
    "    \"Ceuta\":                    \"Ceuta\",\n",
    "    \"Melilla\":                  \"Melilla\",\n",
    "    \"nacional\":                 \"España\"\n",
    "}\n",
    "\n",
    "\n",
    "### 2.1 Charger la demande électrique\n",
    "def load_demand(zone):\n",
    "    df = pd.read_csv(raw_dir/f\"{zone}_hourly.csv\",\n",
    "                     parse_dates=[\"datetime\"],\n",
    "                     index_col=\"datetime\")\n",
    "    df = df.resample(\"h\").sum()\n",
    "    df = df.rename(columns={\"value\": \"demand\"})\n",
    "    return df\n",
    "\n",
    "### 2.2 Charger le prix de l'électricité\n",
    "def load_pvpc(zone: str):\n",
    "    \"\"\"\n",
    "    Charge le fichier PVPC pour `zone`. \n",
    "    Si absent, cherche dans sa région parente.\n",
    "    \"\"\"\n",
    "    filename = f\"{zone}_pvpc_hourly.csv\"\n",
    "    path = ext_dir/\"prices\"/filename\n",
    "    if not path.exists():\n",
    "        parent = parent_region.get(zone)\n",
    "        if parent and parent != zone:\n",
    "            print(f\"⚠️  Pas de fichier pour {zone}, utilisation de {parent} à la place\")\n",
    "            path = ext_dir/\"prices\"/f\"{parent}_pvpc_hourly.csv\"\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Aucun fichier PVPC pour {zone} ni parent\")\n",
    "    df = pd.read_csv(path, parse_dates=[\"datetime\"], index_col=\"datetime\")\n",
    "    df = df.rename(columns={\"value\": \"pvpc\"})\n",
    "    return df\n",
    "\n",
    "### 2.3 les données météo\n",
    "def load_weather(zone):\n",
    "    if zone == \"nacional\":\n",
    "        # On charge les données de la péninsule ibérique\n",
    "        zone = \"Peninsule_Iberique\"\n",
    "    df = pd.read_csv(ext_dir/\"weather\"/f\"{zone}_weather_hourly.csv\",\n",
    "                     parse_dates=[\"datetime\"],\n",
    "                     index_col=\"datetime\")\n",
    "    return df  # colonnes: temperature_2m, relative_humidity_2m, etc.\n",
    "\n",
    "### 2.4 les jours fériés\n",
    "def load_holidays(zone: str):\n",
    "    \"\"\"\n",
    "    Charge le fichier des jours fériés pour `zone`.\n",
    "    Si absent, cherche dans sa région parente.\n",
    "    \"\"\"\n",
    "    if zone == \"nacional\":\n",
    "        zone = \"spain\"\n",
    "    filename = f\"{zone}_holidays.csv\"\n",
    "    path = ext_dir/\"holidays\"/filename\n",
    "    if not path.exists():\n",
    "        parent = parent_region.get(zone)\n",
    "        if parent and parent != zone:\n",
    "            print(f\"⚠️  Pas de fichier pour {zone}, utilisation de {parent} à la place\")\n",
    "            path = ext_dir/\"holidays\"/f\"{parent}_holidays.csv\"\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Aucun fichier de jours fériés pour {zone} ni parent\")\n",
    "    df = pd.read_csv(path, parse_dates=[\"date\"])\n",
    "    df[\"is_holiday\"] = 1\n",
    "    df = df.set_index(\"date\")[[\"is_holiday\"]]\n",
    "    # Localiser l'index en UTC pour matcher df.date\n",
    "    df.index = df.index.tz_localize(\"UTC\")\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e257ab",
   "metadata": {},
   "source": [
    "## 3. Prétraitement horaire et fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c036c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_zone(zone):\n",
    "    # Charger sources\n",
    "    df_dem = load_demand(zone)      # index datetime64[ns]\n",
    "    df_pv  = load_pvpc(zone)        # index datetime64[ns]\n",
    "    df_wth = load_weather(zone)     # index datetime64[ns]\n",
    "    df_hol = load_holidays(zone)    # index date (DatetimeIndex of dates)\n",
    "\n",
    "    # Fusion horaire\n",
    "    df = df_dem.join(df_pv, how=\"left\") \\\n",
    "               .join(df_wth, how=\"left\")\n",
    "\n",
    "    # Créer une colonne 'date' au format datetime64[ns] (00:00)\n",
    "    df[\"date\"] = df.index.normalize()\n",
    "\n",
    "    # Merge sur date, en laissant df_hol index en DatetimeIndex\n",
    "    df = df.merge(\n",
    "        df_hol,\n",
    "        left_on=\"date\",\n",
    "        right_index=True,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    df[\"is_holiday\"] = df[\"is_holiday\"].shift(-2, freq=\"h\") \n",
    "    # Remplir les NaN, supprimer la colonne intermédiaire\n",
    "    df[\"is_holiday\"] = df[\"is_holiday\"].fillna(0).astype(int)\n",
    "    df = df.drop(columns=\"date\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b91bc",
   "metadata": {},
   "source": [
    "## 4. Features temporelles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1206a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_time_features(df):\n",
    "    idx = df.index\n",
    "    df[\"hour\"]      = idx.hour\n",
    "    df[\"dayofweek\"] = idx.dayofweek\n",
    "    df[\"is_weekend\"]= (idx.dayofweek >= 5).astype(int)\n",
    "    df[\"month\"]     = idx.month\n",
    "    df[\"dayofyear\"] = idx.dayofyear\n",
    "    # cyclic encoding\n",
    "    df[\"hour_sin\"]  = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"]  = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff62eff",
   "metadata": {},
   "source": [
    "## 5. Lags & Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c83d616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_rolling_h(df):\n",
    "    df[\"lag_1h\"]  = df[\"demand\"].shift(1)\n",
    "    df[\"lag_2h\"] = df[\"demand\"].shift(2)\n",
    "    df[\"lag_3h\"] = df[\"demand\"].shift(3)\n",
    "    df[\"lag_4h\"] = df[\"demand\"].shift(4)\n",
    "    df[\"lag_5h\"] = df[\"demand\"].shift(5)\n",
    "    df[\"lag_6h\"] = df[\"demand\"].shift(6)\n",
    "    df[\"lag_7h\"] = df[\"demand\"].shift(7)\n",
    "    df[\"lag_8h\"] = df[\"demand\"].shift(8)\n",
    "    df[\"lag_9h\"] = df[\"demand\"].shift(9)\n",
    "    df[\"lag_10h\"] = df[\"demand\"].shift(10)\n",
    "    df[\"lag_11h\"] = df[\"demand\"].shift(11)\n",
    "    df[\"lag_12h\"] = df[\"demand\"].shift(12)\n",
    "    df[\"lag_24h\"] = df[\"demand\"].shift(24)\n",
    "    df[\"lag_168h\"]= df[\"demand\"].shift(168)\n",
    "    df[\"rmean_7d\"]= df[\"demand\"].rolling(24*7).mean()\n",
    "    df[\"rstd_7d\"] = df[\"demand\"].rolling(24*7).std()\n",
    "    return df\n",
    "\n",
    "def add_lag_rolling_d(df):\n",
    "    df[\"lag_1d\"]   = df[\"demand\"].shift(1)\n",
    "    df[\"lag_2d\"]   = df[\"demand\"].shift(2)\n",
    "    df[\"lag_3d\"]   = df[\"demand\"].shift(3)\n",
    "    df[\"lag_4d\"]   = df[\"demand\"].shift(4)\n",
    "    df[\"lag_5d\"]   = df[\"demand\"].shift(5)\n",
    "    df[\"lag_6d\"]   = df[\"demand\"].shift(6)\n",
    "    df[\"lag_7d\"]   = df[\"demand\"].shift(7)\n",
    "    df[\"lag_30d\"]  = df[\"demand\"].shift(30)\n",
    "    df[\"lag_365d\"] = df[\"demand\"].shift(365)\n",
    "    df[\"rmean_7d\"] = df[\"demand\"].rolling(7).mean()\n",
    "    df[\"rstd_7d\"]  = df[\"demand\"].rolling(7).std()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37b66e",
   "metadata": {},
   "source": [
    "## 6. Handling Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e91827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df):\n",
    "    # Forward fill, puis backward for leading NaNs\n",
    "    df = df.ffill().bfill()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19d072",
   "metadata": {},
   "source": [
    "## 7. Pipeline complet & sauvegarde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0e89ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Peninsule_Iberique...\n",
      "  → saved to Peninsule_Iberique_interim_hourly.pkl\n",
      "Processing Baleares...\n",
      "  → saved to Baleares_interim_hourly.pkl\n",
      "Processing Canarias...\n",
      "  → saved to Canarias_interim_hourly.pkl\n",
      "Processing Gran_canaria...\n",
      "⚠️  Pas de fichier pour Gran_canaria, utilisation de Canarias à la place\n",
      "⚠️  Pas de fichier pour Gran_canaria, utilisation de Canarias à la place\n",
      "  → saved to Gran_canaria_interim_hourly.pkl\n",
      "Processing Ceuta...\n",
      "  → saved to Ceuta_interim_hourly.pkl\n",
      "Processing Melilla...\n",
      "  → saved to Melilla_interim_hourly.pkl\n",
      "Processing Lanzarote_Fuerteventura...\n",
      "⚠️  Pas de fichier pour Lanzarote_Fuerteventura, utilisation de Canarias à la place\n",
      "⚠️  Pas de fichier pour Lanzarote_Fuerteventura, utilisation de Canarias à la place\n",
      "  → saved to Lanzarote_Fuerteventura_interim_hourly.pkl\n",
      "Processing Tenerife...\n",
      "⚠️  Pas de fichier pour Tenerife, utilisation de Canarias à la place\n",
      "⚠️  Pas de fichier pour Tenerife, utilisation de Canarias à la place\n",
      "  → saved to Tenerife_interim_hourly.pkl\n",
      "Processing La_Palma...\n",
      "⚠️  Pas de fichier pour La_Palma, utilisation de Canarias à la place\n",
      "⚠️  Pas de fichier pour La_Palma, utilisation de Canarias à la place\n",
      "  → saved to La_Palma_interim_hourly.pkl\n",
      "Processing La_Gomera...\n",
      "⚠️  Pas de fichier pour La_Gomera, utilisation de Canarias à la place\n",
      "⚠️  Pas de fichier pour La_Gomera, utilisation de Canarias à la place\n",
      "  → saved to La_Gomera_interim_hourly.pkl\n",
      "Processing El_Hierro...\n",
      "⚠️  Pas de fichier pour El_Hierro, utilisation de Canarias à la place\n",
      "⚠️  Pas de fichier pour El_Hierro, utilisation de Canarias à la place\n",
      "  → saved to El_Hierro_interim_hourly.pkl\n",
      "Processing nacional...\n",
      "⚠️  Pas de fichier pour nacional, utilisation de España à la place\n",
      "  → saved to nacional_interim_hourly.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for zone in zones:\n",
    "    print(f\"Processing {zone}...\")\n",
    "    df_z = preprocess_zone(zone)\n",
    "    df_z = add_time_features(df_z)\n",
    "    df_z = add_lag_rolling_h(df_z)\n",
    "    df_z = fill_missing(df_z)\n",
    "    # Sauvegarde\n",
    "    out_path = inter_dir/f\"{zone}_interim_hourly.pkl\"\n",
    "\n",
    "    df_daily = df_z.resample(cfg[\"processing\"][\"aggregate_freq\"]).agg({\n",
    "        \"demand\": \"sum\", \n",
    "        \"pvpc\":   \"mean\",\n",
    "        # météo :\n",
    "        \"temperature_2m\":      \"mean\",\n",
    "        \"relative_humidity_2m\":\"mean\",\n",
    "        \"wind_speed_10m\":      \"mean\",\n",
    "        \"shortwave_radiation\": \"sum\",\n",
    "        \"precipitation\":       \"sum\",\n",
    "        # indicateurs temporels :\n",
    "        \"is_holiday\": lambda x: x.mode()[0] if not x.mode().empty else 0,\n",
    "        \"is_weekend\": \"max\"\n",
    "    })\n",
    "\n",
    "    # puis lags journaliers, features date (dayofweek, month…), etc.\n",
    "    df_daily = add_lag_rolling_d(df_daily)\n",
    "    df_daily = fill_missing(df_daily)\n",
    "    \n",
    "    # Sauvegarde\n",
    "    df_z = df_z['2018-05-01':] if zone != \"nacional\" else df_z\n",
    "    df_z.to_pickle(out_path)\n",
    "    print(f\"  → saved to {out_path.name}\")\n",
    "\n",
    "    # df_daily = df_daily['2015-05-01':] if zone != \"nacional\" else df_daily[\"2019-12-28\":]\n",
    "    df_daily.to_pickle(inter_dir/f\"{zone}_interim_daily.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57633a",
   "metadata": {},
   "source": [
    "# Résumé final du prétraitement\n",
    "\n",
    "Ce notebook a permis de réaliser les étapes suivantes pour préparer les données de demande énergétique :\n",
    "\n",
    "- Chargement des données brutes pour chaque zone (demande, prix, météo, jours fériés)\n",
    "- Fusion des différentes sources de données à l’échelle horaire\n",
    "- Création de variables temporelles (heures, jours, week-ends, etc.)\n",
    "- Génération de lags et de moyennes glissantes (rolling) sur la demande\n",
    "- Gestion des valeurs manquantes par interpolation avant/arrière\n",
    "- Agrégation des données à l’échelle journalière avec calcul des indicateurs pertinents\n",
    "- Sauvegarde des jeux de données intermédiaires au format pickle pour chaque zone, en version horaire et journalière\n",
    "\n",
    "Les fichiers générés se trouvent dans :\n",
    "- `data/interim/` : fichiers `{zone}_interim_hourly.pkl` et `{zone}_interim_daily.pkl`\n",
    "\n",
    "Ces fichiers sont prêts pour l’étape suivante de feature engineering et de modélisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
